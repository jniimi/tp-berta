This repository has been modified by [jniimi(@JvckAndersen)](https://x.com/JvckAndersen) to implement on the recent environment.
- Changed the version of the dependencies
- Adding some arguments and functions
- Demo .ipynb notebook for Google Colab [[pretrain](https://colab.research.google.com/github/jniimi/tp-berta/blob/main/demo_pretrain.ipynb)] [[finetune](https://colab.research.google.com/github/jniimi/tp-berta/blob/main/demo_finetune.ipynb)]

# TP-BERTa: A Fundamental LM Adaption Technique to Tabular Data

This repo contains original PyTorch implementation of:

- [Making Pre-trained Language Models Great on Tabular Prediction](https://openreview.net/pdf?id=anzIzGZuLi) (ICLR 2024 Spotlight)

## Key Features

The following key features are proposed in this paper:

- Relative magnitude tokenization (*RMT*): a distributed representation method for continuous values to enhance LM's numerical perception capability.

- Intra-feature attention (*IFA*): a mechanism to pre-fuse feature-wise information for reasonable tabular feature contexts & model acceleration.

- *TP-BERTa*: a resulting LM pre-trained from RoBERTa with the above features for tabular prediction.

## Project Structure

The repo structure and module functions are as follows:

```
â”œâ”€bin ---- // Implementation of tabular models
â”‚â€ƒâ”œâ”€tpberta_modeling.py ---- // TP-BERTa base class
â”‚â€ƒâ””â”€xxx.py ----------------- // Other non-LM DNN baselines
â”œâ”€lib ---- // Utilities
â”‚â€ƒâ”œâ”€aux.py --------------- // Auxiliary Loss: Magnitude-aware Triplet Loss
â”‚â€ƒâ”œâ”€feature_encoder.py --- // Numerical Value Binner (C4.5 discretization)
â”‚â€ƒâ”œâ”€optim.py ------------- // Utilities for optimizer & trainer
â”‚â€ƒâ”œâ”€env.py --------------- // Environment Variables configs
â”‚â€ƒâ”œâ”€data.py -------------- // Dataset & Data Transformation class
â”‚â€ƒâ”œâ”€data_utils.py -------- // Data Config & Multi-task Loader class
â”‚â€ƒâ””â”€xxx.py --------------- // Other standard utils
â”œâ”€data --- // csv file path for pre-training & fine-tuning
â”‚â€ƒâ”œâ”€pretrain-bin
â”‚â€ƒâ”œâ”€pretrain-reg
â”‚â€ƒâ”œâ”€finetune-bin
â”‚â€ƒâ”œâ”€finetune-reg
â”‚â€ƒâ””â”€finetune-mul
â”œâ”€checkpoints --- // Pre-trained model weights & configs (RoBERTa, TP-BERTa)
â”œâ”€configs --- // Model & Training configs for non-LM baselines
â”‚â€ƒâ”œâ”€default --- // default configs
â”‚â€ƒâ””â”€tuned ----- // tuned configs (generated with hyperparameter tuning scripts)
â”œâ”€scripts --- // Experiment codes
â”‚â€ƒâ”œâ”€examples --- // Example shell scripts for main experiments
â”‚â€ƒâ”œâ”€pretrain --- // Codes for TP-BERTa pre-training
â”‚â€ƒâ”œâ”€finetune --- // Codes for baseline fine-tuning & hyperparameter tuning
â”‚â€ƒâ””â”€clean_feat_names.py --- // Text clean for table feature names
```

## Dependencies

All necessary dependencies for TP-BERTa are included in `requirement.txt`. To conduct the packaged baselines, uncomment the corresponding lines.

### How to pre-train a TP-BERTa from scratch

In experiment we saved weights and configs of [RoBERTa-base](https://huggingface.co/FacebookAI/roberta-base/tree/main) in the local `checkpoints/roberta-base` folder (network unavailable) and conducted pre-training with `scripts/pretrain/pretrain_tpberta.py`. You can use online HuggingFace APIs by assigning the argument `--base_model_dir` with "FacebookAI/roberta-base".

### Considerations for fine-tuning a TP-BERTa

The TP-BERTa is designed for standard supervised tabular data prediction, it requires fine-tuning on downstream datasets, and a larger training round (in experiment we uniformly used 200 max training epochs with an early stop of 50 epochs, codes [here](https://github.com/jyansir/tp-berta/blob/main/scripts/finetune/default/run_default_config_tpberta.py)) is preferred compared to the small tabular deep models (e.g., FT-Transformer) since **slightly fine-tuning the BERT-sized model is tend to be underfit** in our practice.

## TODO

- [x] Upload pre-trained TP-BERTa checkpoints.
    1. Download TP-BERTa checkpoints pre-trained on [single task type](https://drive.google.com/uc?export=download&id=13_GAK2VcShxm5TgqSvLk2afBTIYcCbEs) or [both task types](https://drive.google.com/uc?export=download&id=1ArjkOAblGPErmxUyVIfpiM0IztnjjYxq).
    2. Move the `*.tar.gz` file to the `checkpoints` folder (create one if not exists)
    3. Unzip the file and run TP-BERTa according to the scripts in `scripts/examples/finetune`.

- [x] Sort and update experiment datasets.
    1. We have acquired permission on distributing the used data subset from [TabPertNet (OpenTabs currently)](https://arxiv.org/abs/2307.04308) datasets.
    2. Download datasets for [pre-training](https://drive.google.com/uc?export=download&id=1Jy45I_vTKn6McMROi5IKjKoSi9QJtx9A) (202 datasets) and [fine-tuning](https://drive.google.com/uc?export=download&id=1JhOJR1kxjyu4w4ZHi8VcxgMh-iYJRDgG) (145 datasets).
    3. Unzip the `*.tar.gz` file to the `data` folder (create one if not exists).

- [ ] Integrate TP-BERTa to HuggingFaceðŸ¤— community.


## Citation

If you find this useful for your research, please cite the following paper:

```
@article{yan2024making,
  title={Making Pre-trained Language Models Great on Tabular Prediction},
  author={Yan, Jiahuan and Zheng, Bo and Xu, Hongxia and Zhu, Yiheng and Chen, Danny and Sun, Jimeng and Wu, Jian and Chen, Jintai},
  journal={arXiv preprint arXiv:2403.01841},
  year={2024}
}
```

## Acknowledgments

Our codes are influenced by the following repos:

- [HuggingFace Transformers](https://github.com/huggingface/transformers)
- [RTDL Numerical Embeddings](https://github.com/yandex-research/rtdl-num-embeddings)