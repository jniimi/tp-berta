{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true,
      "machine_shape": "hm",
      "mount_file_id": "1Cn1Jzlt06XIWIqXLRXisgoprjJSORT_K",
      "authorship_tag": "ABX9TyPKn5Z9rU0PbaSuNn4cAAPj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jniimi/tp-berta/blob/main/demo_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the Environment\n",
        "- Clone repository from [jniimi/tp-berta](https://github.com/jniimi/tp-berta)\n",
        "- Install required packages from pip\n",
        "- It is quite tough for Colab to solve all the dependencies in requirements.txt\n",
        "- The version of NumPy may need to be less than 1.24"
      ],
      "metadata": {
        "id": "rTELxBSaq6EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jniimi/tp-berta\n",
        "!pip install -q numpy==1.23.5 pandas scikit_learn torch torchvision torchaudio transformers tqdm wandb gdown category_encoders tomli tomli_w einops"
      ],
      "metadata": {
        "id": "_be8tTCc7q92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download model checkpoints\n",
        "- Directly download the pre-trained model checkpoints from Google Drive (the files are officially provided by the original authors)\n",
        "- Extract the compressed files\n",
        "- Remove compressed files"
      ],
      "metadata": {
        "id": "y8T3zmLGf41A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tp-berta\n",
        "!mkdir checkpoints\n",
        "%cd checkpoints\n",
        "!gdown 13_GAK2VcShxm5TgqSvLk2afBTIYcCbEs\n",
        "!gdown 1ArjkOAblGPErmxUyVIfpiM0IztnjjYxq\n",
        "!tar -xzvf tpberta-single.tar.gz\n",
        "!tar -xzvf tpberta-joint.tar.gz\n",
        "!rm tpberta-single.tar.gz\n",
        "!rm tpberta-joint.tar.gz\n",
        "%cd /content/tp-berta"
      ],
      "metadata": {
        "id": "YmNKfSMI_eJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download datasets\n",
        "- Directly download datasets from Google Drive (the files are officially provided by the original authors)\n",
        "- Extract the compressed files\n",
        "- Remove compressed files"
      ],
      "metadata": {
        "id": "YjsFhoZjf6mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tp-berta\n",
        "!mkdir data\n",
        "%cd data\n",
        "!gdown 1Jy45I_vTKn6McMROi5IKjKoSi9QJtx9A\n",
        "!gdown 1JhOJR1kxjyu4w4ZHi8VcxgMh-iYJRDgG\n",
        "!tar -xzvf tpberta-finetune-data.tar.gz\n",
        "!tar -xzvf tpberta-pretrain-data.tar.gz\n",
        "!rm tpberta-finetune-data.tar.gz\n",
        "!rm tpberta-pretrain-data.tar.gz\n",
        "%cd /content/tp-berta"
      ],
      "metadata": {
        "id": "Rtn-lp1D_eq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Datasets\n",
        "Different from the preprocession in the pre-training, it is recommended to implement all the datasets in the finetuning preparation.\n",
        "\n",
        "Still, you can use following arguments:\n",
        "\n",
        "- **num_datasets (int)**:  \n",
        "  Randomly sample a smaller number of datasets for quick pre-training attempts.  \n",
        "  (Useful since the default full dataset is very large and takes time to prepare.)\n",
        "\n",
        "- **dataset_seed (int)**:  \n",
        "  Set the random seed value for sampling datasets. Ensures reproducibility."
      ],
      "metadata": {
        "id": "CBom3ygcgDpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tp-berta\n",
        "#!python scripts/clean_feat_names.py --mode finetune --task binclass --num_datasets 3 --dataset_seed 123 --overwrite\n",
        "!python scripts/clean_feat_names.py --mode finetune --task binclass --overwrite"
      ],
      "metadata": {
        "id": "6XvPeIvQAek-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune\n",
        "- **Option 1. Explicitly specify the dataset filename**\n",
        "\n",
        "  - The dataset file (.csv) needs to be put in `tp-berta/data/finetune-{task}` directory, such as `finetune-bin`.\n",
        "  - List the datasets (without extention: .csv) in `DATASETS` and automatically finetune the model in the loop.\n",
        "\n",
        "- **Option 2. Randomly sample the datasets**\n",
        "  - You can also randomly select the datasets for finetuning.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "_468D3BnqRT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "task = 'bin'\n",
        "\n",
        "# Option 1: Specify the dataset\n",
        "#DATASETS=[\"Customer_Behaviour\", \"Bank_Personal_Loan_Modelling\"]\n",
        "\n",
        "# Option 2: Random sampling\n",
        "num_datasets = 2\n",
        "dataset_seed = 123\n",
        "DATASETS = [f.replace('.csv','') for f in os.listdir(f'/content/tp-berta/data/finetune-{task}') if f.endswith('.csv')]\n",
        "datasets_rng = random.Random(dataset_seed)\n",
        "DATASETS = datasets_rng.sample(DATASETS, k=num_datasets)\n",
        "\n",
        "for DATASET in DATASETS:\n",
        "    !python scripts/finetune/default/run_default_config_tpberta.py --dataset \"$DATASET\" --task \"binclass\""
      ],
      "metadata": {
        "id": "xCNWlb1vqS2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the finetuned model is saved in / content/tp-berta/finetune_outputs/."
      ],
      "metadata": {
        "id": "9gGsBGMQz5rx"
      }
    }
  ]
}