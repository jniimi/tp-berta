{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true,
      "machine_shape": "hm",
      "mount_file_id": "1Cn1Jzlt06XIWIqXLRXisgoprjJSORT_K",
      "authorship_tag": "ABX9TyMnICRlUe3Gek2njbXYwpWM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jniimi/tp-berta/blob/main/demo_pretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the Environment\n",
        "- Clone repository from [jniimi/tp-berta](https://github.com/jniimi/tp-berta)\n",
        "- Install required packages from pip\n",
        "- It is quite tough for Colab to solve all the dependencies in requirements.txt\n",
        "- The version of NumPy may need to be less than 1.24"
      ],
      "metadata": {
        "id": "rTELxBSaq6EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jniimi/tp-berta\n",
        "!pip install -q numpy==1.23.5 pandas scikit_learn torch torchvision torchaudio transformers tqdm wandb gdown category_encoders tomli tomli_w einops"
      ],
      "metadata": {
        "id": "_be8tTCc7q92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download datasets\n",
        "- Directly download datasets from Google Drive (the files are officially provided by the original authors)\n",
        "- Extract the compressed files\n",
        "- Remove compressed files"
      ],
      "metadata": {
        "id": "YjsFhoZjf6mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tp-berta\n",
        "!mkdir data\n",
        "%cd data\n",
        "!gdown 1Jy45I_vTKn6McMROi5IKjKoSi9QJtx9A\n",
        "!gdown 1JhOJR1kxjyu4w4ZHi8VcxgMh-iYJRDgG\n",
        "!tar -xzvf tpberta-finetune-data.tar.gz\n",
        "!tar -xzvf tpberta-pretrain-data.tar.gz\n",
        "!rm tpberta-finetune-data.tar.gz\n",
        "!rm tpberta-pretrain-data.tar.gz\n",
        "%cd /content/tp-berta"
      ],
      "metadata": {
        "id": "Rtn-lp1D_eq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Datasets\n",
        "You can optionally use these arguments:\n",
        "\n",
        "- **num_datasets (int)**:  \n",
        "  Randomly sample a smaller number of datasets for quick pre-training attempts.  \n",
        "  (Useful since the default full dataset is very large and takes time to prepare.)\n",
        "\n",
        "- **dataset_seed (int)**:  \n",
        "  Set the random seed value for sampling datasets. Ensures reproducibility."
      ],
      "metadata": {
        "id": "CBom3ygcgDpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tp-berta\n",
        "!python scripts/clean_feat_names.py --mode pretrain --task binclass --num_datasets 3 --dataset_seed 123 --overwrite"
      ],
      "metadata": {
        "id": "6XvPeIvQAek-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain\n",
        "You can optionally use these arguments:\n",
        "- **device (str)**:\n",
        "  Set the PyTorch training device: `'cpu'`, `'cuda'`, or `'mps'`.\n",
        "\n",
        "**Note**: It's recommended to use a `batch_size` smaller than 512, as larger sizes can cause crashes even on high-end GPUs (e.g., A100).\n",
        "\n",
        "Additionally, the environment variable `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` is explicitly set to optimize CUDA memory allocation."
      ],
      "metadata": {
        "id": "L8MMgE__gHaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python scripts/pretrain/pretrain_tpberta.py --batch_size 256 --device cuda --task binclass --base_model_dir FacebookAI/roberta-base"
      ],
      "metadata": {
        "id": "dP_Y27jxgG2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the trained model is saved in / content/tp-berta/pretrain_outputs/."
      ],
      "metadata": {
        "id": "uHdasTxisGgX"
      }
    }
  ]
}